{"metadata":{"kernelspec":{"name":"glue_pyspark","display_name":"Glue PySpark","language":"python"},"language_info":{"name":"Python_Glue_Session","mimetype":"text/x-python","codemirror_mode":{"name":"python","version":3},"pygments_lexer":"python3","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n","metadata":{"editable":true,"trusted":true}},{"cell_type":"markdown","source":"#### Optional: Run this cell to see available notebook commands (\"magics\").\n","metadata":{"editable":true,"trusted":true}},{"cell_type":"code","source":"# %help","metadata":{"trusted":true,"editable":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \n","output_type":"stream"}]},{"cell_type":"markdown","source":"####  Run this cell to set up and start your interactive session.\n","metadata":{"editable":true,"trusted":true}},{"cell_type":"code","source":"%idle_timeout 2880\n%glue_version 4.0\n%worker_type G.1X\n%number_of_workers 5\n\nimport sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n\nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)","metadata":{"trusted":true,"editable":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \nCurrent idle_timeout is None minutes.\nidle_timeout has been set to 2880 minutes.\nSetting Glue version to: 4.0\nPrevious worker type: None\nSetting new worker type to: G.1X\nPrevious number of workers: None\nSetting new number of workers to: 5\nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nWorker Type: G.1X\nNumber of Workers: 5\nIdle Timeout: 2880\nSession ID: db67fb82-2499-41dc-8a7a-67dbcd0ed93e\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\nWaiting for session db67fb82-2499-41dc-8a7a-67dbcd0ed93e to get into ready status...\nSession db67fb82-2499-41dc-8a7a-67dbcd0ed93e has been created.\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# important! using python min, max won't work\nfrom pyspark.sql.functions import col, min, max, sum, avg, count, countDistinct, row_number\nfrom pyspark.sql.window import Window\n\n# https://spark.apache.org/docs/latest/sql-ref-datatypes.html\nfrom pyspark.sql.types import StructType, StructField, BooleanType, ByteType, ShortType, IntegerType, StringType, FloatType, DoubleType","metadata":{"trusted":true,"tags":[]},"execution_count":7,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## aisles\nread as csv, save as parquet, then read from parquet","metadata":{}},{"cell_type":"code","source":"aisles_schema = StructType([\n    StructField(\"aisle_id\", IntegerType(), True),\n    StructField(\"aisle\", StringType(), True)\n])\naisles = spark.read.csv(\"s3://sam-raw/aisles/aisles.csv\", header=True, schema=aisles_schema)\naisles.write.mode(\"overwrite\").parquet(\"s3://sam-raw-parquet/aisles/\")\naisles = spark.read.parquet('s3://sam-raw-parquet/aisles')\naisles.printSchema()\naisles.count()","metadata":{"trusted":true,"tags":[]},"execution_count":5,"outputs":[{"name":"stdout","text":"root\n |-- aisle_id: integer (nullable = true)\n |-- aisle: string (nullable = true)\n\n134\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## departments\nread as csv, save as parquet, then read from parquet","metadata":{}},{"cell_type":"code","source":"departments_schema = StructType([\n    StructField(\"department_id\", IntegerType(), True),\n    StructField(\"department\", StringType(), True)\n])\ndepartments = spark.read.csv(\"s3://sam-raw/departments/departments.csv\", header=True, schema=departments_schema)\ndepartments.write.mode(\"overwrite\").parquet(\"s3://sam-raw-parquet/departments/\")\ndepartments = spark.read.parquet('s3://sam-raw-parquet/departments') # read as parquet\ndepartments.printSchema()\ndepartments.count()","metadata":{"trusted":true,"tags":[]},"execution_count":6,"outputs":[{"name":"stdout","text":"root\n |-- department_id: integer (nullable = true)\n |-- department: string (nullable = true)\n\n21\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## products\nread as csv, save as parquet, then read from parquet","metadata":{}},{"cell_type":"code","source":"products_schema = StructType([\n    StructField(\"product_id\", IntegerType(), True),\n    StructField(\"product_name\", StringType(), True),\n    StructField(\"aisle_id\", IntegerType(), True),\n    StructField(\"department_id\", IntegerType(), True)\n])\nproducts = spark.read.csv(\"s3://sam-raw/products/products.csv\", header=True, schema=products_schema)\nproducts.write.mode(\"overwrite\").parquet(\"s3://sam-raw-parquet/products/\")\nproducts = spark.read.parquet('s3://sam-raw-parquet/products') # read as parquet\nproducts.printSchema()\nproducts.count()","metadata":{"trusted":true,"tags":[]},"execution_count":7,"outputs":[{"name":"stdout","text":"root\n |-- product_id: integer (nullable = true)\n |-- product_name: string (nullable = true)\n |-- aisle_id: integer (nullable = true)\n |-- department_id: integer (nullable = true)\n\n49688\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## denorm products\njoin with aisles and departments, save to transformed","metadata":{"tags":[]}},{"cell_type":"code","source":"products_denorm = products\\\n                    .join(aisles, products.aisle_id==aisles.aisle_id, 'inner')\\\n                    .join(departments, products.department_id==departments.department_id, 'inner')\\\n                    .select(products.product_id,\n                            products.product_name,\n                            products.aisle_id,\n                            aisles.aisle,\n                            products.department_id,\n                            departments.department\n                           )\nproducts_denorm.printSchema()\nproducts_denorm.write.mode(\"overwrite\").parquet(\"s3://sam-transformed/products/\")","metadata":{"trusted":true,"tags":[]},"execution_count":11,"outputs":[{"name":"stdout","text":"root\n |-- product_id: integer (nullable = true)\n |-- product_name: string (nullable = true)\n |-- aisle_id: integer (nullable = true)\n |-- aisle: string (nullable = true)\n |-- department_id: integer (nullable = true)\n |-- department: string (nullable = true)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## orders\nread as csv, partition by eval_set, save as parquet, then read from parque","metadata":{}},{"cell_type":"code","source":"orders_schema = StructType([\n    StructField(\"order_id\", IntegerType(), True),\n    StructField(\"user_id\", IntegerType(), True),\n    StructField(\"eval_set\", StringType(), True),\n    StructField(\"order_number\", IntegerType(), True),\n    StructField(\"order_dow\", ByteType(), True),\n    StructField(\"order_hour_of_day\", ByteType(), True),\n    StructField(\"days_since_prior_order\", FloatType(), True)\n])\norders = spark.read.csv(\"s3://sam-raw/orders/orders.csv\", header=True, schema=orders_schema)\norders.write.partitionBy(\"eval_set\").mode(\"overwrite\").parquet(\"s3://sam-raw-parquet/orders/\")\norders = spark.read.parquet('s3://sam-raw-parquet/orders') # read as parquet\norders.printSchema()\nprint(orders.count())\norders.agg(min('order_number'), max('order_number')).show()\norders.agg(min('days_since_prior_order'), max('days_since_prior_order')).show()","metadata":{"trusted":true,"tags":[]},"execution_count":18,"outputs":[{"name":"stdout","text":"root\n |-- order_id: integer (nullable = true)\n |-- user_id: integer (nullable = true)\n |-- order_number: integer (nullable = true)\n |-- order_dow: byte (nullable = true)\n |-- order_hour_of_day: byte (nullable = true)\n |-- days_since_prior_order: float (nullable = true)\n |-- eval_set: string (nullable = true)\n\n3421083\n+-----------------+-----------------+\n|min(order_number)|max(order_number)|\n+-----------------+-----------------+\n|                1|              100|\n+-----------------+-----------------+\n\n+---------------------------+---------------------------+\n|min(days_since_prior_order)|max(days_since_prior_order)|\n+---------------------------+---------------------------+\n|                        0.0|                       30.0|\n+---------------------------+---------------------------+\n","output_type":"stream"}]},{"cell_type":"code","source":"# filter by eval_set=prior\norders_prior = orders.where(orders.eval_set=='prior').select(*[c for c in orders.columns if c!='eval_set'])\nprint(orders_prior.count())\norders_prior.write.mode(\"overwrite\").parquet(\"s3://sam-transformed/orders_prior/\")","metadata":{"trusted":true,"tags":[]},"execution_count":18,"outputs":[{"name":"stdout","text":"3214874\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## order_products\nread as csv, save as parquet, then read from parque","metadata":{}},{"cell_type":"code","source":"# takes 1 minute to run\norder_products_schema = StructType([\n    StructField(\"order_id\", IntegerType(), True),\n    StructField(\"product_id\", IntegerType(), True),\n    StructField(\"add_to_cart_order\", IntegerType(), True),\n    StructField(\"reordered\", IntegerType(), True)\n])\norder_products = spark.read.csv(\"s3://sam-raw/order_products\", header=True, schema=order_products_schema)\norder_products = order_products.withColumn(\"reordered\", col(\"reordered\").cast(\"boolean\"))\norder_products.write.mode(\"overwrite\").parquet(\"s3://sam-raw-parquet/order_products/\")\norder_products = spark.read.parquet('s3://sam-raw-parquet/order_products') # read as parquet\norder_products.printSchema()\norder_products.count()","metadata":{"trusted":true,"tags":[]},"execution_count":14,"outputs":[{"name":"stdout","text":"root\n |-- order_id: integer (nullable = true)\n |-- product_id: integer (nullable = true)\n |-- add_to_cart_order: integer (nullable = true)\n |-- reordered: boolean (nullable = true)\n\n33819106\n","output_type":"stream"}]},{"cell_type":"code","source":"# takes 20 seconds to run\norder_products_prior = orders_prior\\\n                        .join(order_products, orders_prior.order_id==order_products.order_id, 'inner')\\\n                        .select(orders_prior.order_id,\n                                orders_prior.user_id,\n                                orders_prior.order_number,\n                                orders_prior.order_dow,\n                                orders_prior.order_hour_of_day,\n                                orders_prior.days_since_prior_order,\n                                order_products.product_id,\n                                order_products.add_to_cart_order,\n                                order_products.reordered\n                               )\norder_products_prior.write.mode(\"overwrite\").parquet(\"s3://sam-transformed/order_products_prior/\")","metadata":{"trusted":true,"tags":[]},"execution_count":19,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Q2\n```sql\nselect \n    user_id, \n    max(order_number) as max_order_number, \n    sum(days_since_prior_order) as sum_days_since_prior_order, \n    avg(days_since_prior_order) as avg_days_since_prior_order\nfrom orders\ngroup by user_id;\n```","metadata":{}},{"cell_type":"code","source":"# orders = spark.read.parquet('s3://sam-raw-parquet/orders') # read as parquet","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_features_1 = orders.groupBy('user_id').agg(max('order_number').alias('max_order_number'),\n                                               sum('days_since_prior_order').alias('sum_days_since_prior_order'),\n                                               avg('days_since_prior_order').alias('avg_days_since_prior_order')\n                                               )\nuser_features_1.orderBy('user_id').show(10)\n# save aggregated result as one part\nuser_features_1.write.mode(\"overwrite\").parquet(\"s3://sam-transformed/user_features_1/\")","metadata":{"trusted":true,"tags":[]},"execution_count":23,"outputs":[{"name":"stdout","text":"+-------+----------------+--------------------------+--------------------------+\n|user_id|max_order_number|sum_days_since_prior_order|avg_days_since_prior_order|\n+-------+----------------+--------------------------+--------------------------+\n|      1|              11|                     190.0|                      19.0|\n|      2|              15|                     228.0|        16.285714285714285|\n|      3|              13|                     144.0|                      12.0|\n|      4|               6|                      85.0|                      17.0|\n|      5|               5|                      46.0|                      11.5|\n|      6|               4|                      40.0|        13.333333333333334|\n|      7|              21|                     209.0|                     10.45|\n|      8|               4|                      70.0|        23.333333333333332|\n|      9|               4|                      66.0|                      22.0|\n|     10|               6|                     109.0|                      21.8|\n+-------+----------------+--------------------------+--------------------------+\nonly showing top 10 rows\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Q3\n```sql\nSELECT\n    user_id,\n    COUNT(product_id) AS total_products_count,\n    COUNT(DISTINCT product_id) AS total_distinct_products_count, \n    SUM(CASE WHEN reordered = 1 THEN 1 ELSE 0 END) * 1.0 / \n    SUM(CASE WHEN order_number > 1 THEN 1 ELSE 0 END) AS reorder_ratio\nFROM order_products_prior\nGROUP BY user_id;\n```","metadata":{}},{"cell_type":"code","source":"# order_products_prior = spark.read.parquet('s3://sam-transformed/order_products_prior') # read as parquet","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_features_2 = order_products_prior.groupBy('user_id').agg(count('product_id').alias('total_products'),\n                                                              countDistinct('product_id').alias('total_distinct_products'),\n                                                              (sum(col('reordered').cast('int'))/\n                                                               sum((col('order_number')>1).cast('int'))).alias('reorder_ratio')\n                                                            )\nuser_features_2.orderBy('user_id').show(10)\nuser_features_2.write.mode(\"overwrite\").parquet(\"s3://sam-transformed/user_features_2/\")","metadata":{"trusted":true,"tags":[]},"execution_count":24,"outputs":[{"name":"stdout","text":"+-------+--------------+-----------------------+-------------------+\n|user_id|total_products|total_distinct_products|      reorder_ratio|\n+-------+--------------+-----------------------+-------------------+\n|      1|            59|                     18| 0.7592592592592593|\n|      2|           195|                    102|  0.510989010989011|\n|      3|            88|                     33| 0.7051282051282052|\n|      4|            18|                     17|0.07142857142857142|\n|      5|            37|                     23| 0.5384615384615384|\n|      6|            14|                     12|                0.2|\n|      7|           206|                     68|  0.711340206185567|\n|      8|            49|                     36| 0.4642857142857143|\n|      9|            76|                     58|  0.391304347826087|\n|     10|           143|                     94|0.35507246376811596|\n+-------+--------------+-----------------------+-------------------+\nonly showing top 10 rows\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Q4\n```sql\nSELECT\n    user_id,\n    product_id,\n    COUNT(order_id) AS total_orders,\n    MIN(order_number) AS min_order_number,\n    MAX(order_number) AS max_order_number,\n    AVG(add_to_cart_order) AS avg_add_to_cart_order\nFROM order_products_prior\nGROUP BY user_id, product_id;\n```","metadata":{}},{"cell_type":"code","source":"up_features = order_products_prior.groupBy('user_id', 'product_id').agg(count('order_id').alias('total_orders'),\n                                                                        min('order_number').alias('min_order_number'),\n                                                                        max('order_number').alias('max_order_number'),\n                                                                        avg('add_to_cart_order').alias('avg_add_to_cart_order')\n                                                                       )\nup_features.orderBy('user_id', 'product_id').show(10)\nup_features.write.mode(\"overwrite\").parquet(\"s3://sam-transformed/up_features/\")","metadata":{"trusted":true,"tags":[]},"execution_count":25,"outputs":[{"name":"stdout","text":"+-------+----------+------------+----------------+----------------+---------------------+\n|user_id|product_id|total_orders|min_order_number|max_order_number|avg_add_to_cart_order|\n+-------+----------+------------+----------------+----------------+---------------------+\n|      1|       196|          10|               1|              10|                  1.4|\n|      1|     10258|           9|               2|              10|   3.3333333333333335|\n|      1|     10326|           1|               5|               5|                  5.0|\n|      1|     12427|          10|               1|              10|                  3.3|\n|      1|     13032|           3|               2|              10|    6.333333333333333|\n|      1|     13176|           2|               2|               5|                  6.0|\n|      1|     14084|           1|               1|               1|                  2.0|\n|      1|     17122|           1|               5|               5|                  6.0|\n|      1|     25133|           8|               3|              10|                  4.0|\n|      1|     26088|           2|               1|               2|                  4.5|\n+-------+----------+------------+----------------+----------------+---------------------+\nonly showing top 10 rows\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Q5\n```sql\nSELECT \n    product_id,\n    COUNT(product_id) AS total_products,\n    SUM(reordered) AS total_reordered,\n    SUM(CASE WHEN product_seq_time = 1 THEN 1 ELSE 0 END) AS product_seq_time_is_1,\n    SUM(CASE WHEN product_seq_time = 2 THEN 1 ELSE 0 END) AS product_seq_time_is_2\nFROM (\n    SELECT\n        product_id,\n        reordered,\n        ROW_NUMBER() OVER (PARTITION BY user_id, product_id ORDER BY order_number ASC) AS product_seq_time\n    FROM order_products_prior\n) prod_seq\nGROUP BY product_id;\n```","metadata":{}},{"cell_type":"code","source":"prod_seq = order_products_prior.withColumn('product_seq_time', \n                                           row_number().over(Window\\\n                                                             .partitionBy('user_id', 'product_id')\\\n                                                             .orderBy(col('order_number').asc())\n                                                            )\n                                          ).select('product_id', 'reordered', 'product_seq_time')\n\nprd_features = prod_seq.groupBy('product_id').agg(count('product_id').alias('total_products'),\n                                                  sum(col('reordered').cast('int')).alias('total_reordered'),\n                                                  sum((col('product_seq_time')==1).cast('int')).alias('product_seq_time_is_1'),\n                                                  sum((col('product_seq_time')==2).cast('int')).alias('product_seq_time_is_2')\n                                                 )\nprd_features.orderBy('product_id').show(10)\nprd_features.write.mode(\"overwrite\").parquet(\"s3://sam-transformed/prd_features/\")","metadata":{"trusted":true,"tags":[]},"execution_count":26,"outputs":[{"name":"stdout","text":"+----------+--------------+---------------+---------------------+---------------------+\n|product_id|total_products|total_reordered|product_seq_time_is_1|product_seq_time_is_2|\n+----------+--------------+---------------+---------------------+---------------------+\n|         1|          1852|           1136|                  716|                  276|\n|         2|            90|             12|                   78|                    8|\n|         3|           277|            203|                   74|                   36|\n|         4|           329|            147|                  182|                   64|\n|         5|            15|              9|                    6|                    4|\n|         6|             8|              3|                    5|                    2|\n|         7|            30|             12|                   18|                    6|\n|         8|           165|             83|                   82|                   30|\n|         9|           156|             82|                   74|                   31|\n|        10|          2572|           1304|                 1268|                  399|\n+----------+--------------+---------------+---------------------+---------------------+\nonly showing top 10 rows\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}